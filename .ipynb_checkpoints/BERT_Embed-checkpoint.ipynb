{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Concatenate, Layer, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Embedding, Flatten,LeakyReLU,ReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from functools import partial\n",
    "from gumbel_softmax import GumbelSoftmax\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "pd.options.display.max_rows = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = tf.config.experimental.get_visible_devices('GPU')[0]\n",
    "tf.config.experimental.set_memory_growth(device = gpu, enable = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_SC_seq = np.load('Data/y_train_SC_seq.npy',allow_pickle=True)\n",
    "y_test_SC_seq = np.load('Data/y_test_SC_seq.npy',allow_pickle=True)\n",
    "y_train_seq = np.load('Data/y_train_seq.npy',allow_pickle=True)\n",
    "y_test_seq = np.load('Data/y_test_seq.npy',allow_pickle=True)\n",
    "\n",
    "y_train_nseq= pd.read_csv('Data/y_train_nseq.csv')\n",
    "y_test_nseq= pd.read_csv('Data/y_test_nseq.csv')\n",
    "y_train_SC_nseq= pd.read_csv('Data/y_train_SC_nseq.csv')\n",
    "y_test_SC_nseq= pd.read_csv('Data/y_test_SC_nseq.csv')\n",
    "x_train_cond= pd.read_csv('Data/x_train_cond.csv')\n",
    "x_test_cond= pd.read_csv('Data/x_test_cond.csv')\n",
    "\n",
    "## Qualitative attrubutes in the raw small-scale complete data\n",
    "x_train_cond_R = pd.read_csv('Data/train_complete_qualitative.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define function and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make Ground Truth & Test\n",
    "n_uni_col = [x_train_cond_R[i].nunique() for i in x_train_cond_R.columns[1:7]]\n",
    "n_uni_col = [0]+n_uni_col+[6,6,6,6,6]\n",
    "n_uni_col = np.cumsum(n_uni_col)\n",
    "col_pop = x_test_cond.columns\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "n_col = [x_train_cond_R[i].nunique() for i in x_train_cond_R.columns[1:7]]\n",
    "n_col = n_col+[6,6,6,6,6]\n",
    "emb_col = [4,2,2,9,2,4,4,4,4,4,4]\n",
    "\n",
    "\n",
    "def wide_to_long(samples_pop):\n",
    "    resamples = []\n",
    "    for j in range(samples_pop.shape[0]):\n",
    "        if(type(samples_pop) is np.ndarray):\n",
    "            sam = samples_pop[j]\n",
    "        else:\n",
    "            sam = samples_pop.values[j]\n",
    "        resamples_row = []\n",
    "        for i in range(len(n_uni_col)-1):\n",
    "            idx = range(n_uni_col[i],n_uni_col[i+1])\n",
    "            resamples_row = np.append(resamples_row,np.random.choice(col_pop[idx],p=sam[idx],size=1))\n",
    "        resamples = np.concatenate((resamples,resamples_row),axis=0)\n",
    "    resamples = resamples.reshape(samples_pop.shape[0],len(n_uni_col)-1 )\n",
    "    resamples = pd.DataFrame(resamples,columns= x_train_cond_R.columns[1:7].to_list()+[\"TP_0\",\"TP_1\",\"TP_2\",\"TP_3\",\"TP_4\"])\n",
    "    resamples = resamples.apply(lambda x: x.astype('category'))\n",
    "    return(resamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transform the data into Label form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Embedding Networks\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "x_Bert_train = wide_to_long(x_train_cond)\n",
    "x_Bert_test = wide_to_long(x_test_cond)\n",
    "\n",
    "\n",
    "def Multi_LabelEncoder(x_Bert_train,x_Bert_test):\n",
    "    output_train = x_Bert_train.copy()\n",
    "    output_test = x_Bert_test.copy()\n",
    "    for col in x_Bert_test.columns:\n",
    "        LE = LabelEncoder().fit(x_Bert_train[col])\n",
    "        output_train[col] = LE.transform(x_Bert_train[col])\n",
    "        output_test[col] = LE.transform(x_Bert_test[col])\n",
    "    \n",
    "    return output_train,output_test\n",
    "\n",
    "x_Bert_train_lab,x_Bert_test_lab = Multi_LabelEncoder(x_Bert_train,x_Bert_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define the function for BERT networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 11 #256\n",
    "VOCAB_SIZE = 16 #30000\n",
    "\n",
    "\n",
    "def get_masked_input_and_labels(x_Bert_train_lab):\n",
    "       \n",
    "    x_Bert_train_lab = np.array(x_Bert_train_lab)\n",
    "    # 15% BERT masking\n",
    "    inp_mask = np.random.rand(*x_Bert_train_lab.shape) < 0.15\n",
    "    # Set targets to -1 by default, it means ignore\n",
    "    labels = -1 * np.ones(x_Bert_train_lab.shape, dtype=int)\n",
    "    # Set labels for masked tokens\n",
    "    labels[inp_mask] = x_Bert_train_lab[inp_mask]\n",
    "\n",
    "    # Prepare input\n",
    "    x_Bert_train_lab_masked = np.copy(x_Bert_train_lab)\n",
    "    \n",
    "    # This means leaving 10% unchanged\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand(*x_Bert_train_lab.shape) < 0.90)\n",
    "    x_Bert_train_lab_masked[\n",
    "        inp_mask_2mask\n",
    "    ] = 16  # mask token is the last in the dict\n",
    "    \n",
    "    for i in range(x_Bert_train_lab_masked.shape[1]):\n",
    "        idx = x_Bert_train_lab_masked[:,i] == 16\n",
    "        x_Bert_train_lab_masked[:,i][idx] = (n_col[i]-1)\n",
    "\n",
    "\n",
    "    # Prepare sample_weights to pass to .fit() method\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    sample_weights[labels == -1] = 0\n",
    "\n",
    "    # y_labels would be same as encoded_texts i.e input tokens\n",
    "    y_labels = np.copy(x_Bert_train_lab)\n",
    "\n",
    "    return x_Bert_train_lab_masked, y_labels, sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define the Masked Language Model (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Masked Language Model\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    reduction=tf.keras.losses.Reduction.NONE\n",
    ")\n",
    "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "\n",
    "class MaskedLanguageModel(tf.keras.Model):\n",
    "    def train_step(self, inputs):\n",
    "        if len(inputs) == 3:\n",
    "            features, labels, sample_weight = inputs\n",
    "        else:\n",
    "            features, labels = inputs\n",
    "            sample_weight = None\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=True)\n",
    "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [loss_tracker]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embed_model():\n",
    "    \n",
    "    inputs = []\n",
    "    embeddings = []\n",
    "    for i in range(x_Bert_train_lab.shape[1]):\n",
    "        k_embedding_input = Input((1,),dtype=tf.int64)\n",
    "        inputs.append(k_embedding_input)\n",
    "\n",
    "        k_embedding = Embedding(input_dim = n_col[i],\n",
    "                                output_dim = emb_col[i],\n",
    "                                name = 'embedding'+'_'+np.str(i))(k_embedding_input)\n",
    "        embeddings.append(k_embedding)\n",
    "        \n",
    "\n",
    "    embeddings = Concatenate()(embeddings)\n",
    "\n",
    "    dense1 = Dense(32,activation='relu')(embeddings)\n",
    "    dense2 = Dense(32,activation='relu')(dense1)\n",
    "   \n",
    "    outputs = []\n",
    "    for i in range(x_Bert_train_lab.shape[1]):\n",
    "        k_output = Dense(n_col[i],activation='softmax',name='out'+np.str(i))(dense2)\n",
    "        outputs.append(k_output)\n",
    "    \n",
    "    mlm_model = tf.keras.Model(inputs,outputs,name=\"masked_model\")\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    mlm_model.compile(optimizer=optimizer,loss=\"SparseCategoricalCrossentropy\")\n",
    "    \n",
    "    return mlm_model\n",
    "\n",
    "#bert_masked_model = create_masked_language_bert_model()\n",
    "bert_masked_model = create_embed_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training the BERT model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
    "    x_Bert_train_lab\n",
    ")\n",
    "\n",
    "x_train = [x_masked_train.transpose()[i] for i in range(11)]\n",
    "y_labels = [y_masked_labels.transpose()[i] for i in range(11)]\n",
    "weights = [sample_weights.transpose()[i] for i in range(11)]\n",
    "\n",
    "sample_weight = {'out0': weights[0],\n",
    "                 'out1': weights[1],\n",
    "                 'out2': weights[2],\n",
    "                 'out3': weights[3],\n",
    "                 'out4': weights[4],\n",
    "                 'out5': weights[5],\n",
    "                 'out6': weights[6],\n",
    "                 'out7': weights[7],\n",
    "                 'out8': weights[8],\n",
    "                 'out9': weights[9],\n",
    "                 'out10': weights[10]}\n",
    "\n",
    "\n",
    "bert_masked_model.fit(x=x_train,\n",
    "                      y=y_labels,\n",
    "                       shuffle=True,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      sample_weight=sample_weight,\n",
    "                      epochs=100,\n",
    "                      verbose=2)\n",
    "# bert_masked_model.fit(mlm_ds, epochs=50)\n",
    "bert_masked_model.save(\"MLM_Embed_indiv.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the BERT model (Meaningless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7252875014053181\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "x_masked_test, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
    "    x_Bert_test_lab\n",
    ")\n",
    "\n",
    "x_test = [x_masked_test.transpose()[i] for i in range(11)]\n",
    "y_labels = [y_masked_labels.transpose()[i] for i in range(11)]\n",
    "weights = [sample_weights.transpose()[i] for i in range(11)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load pretrained bert model\n",
    "mlm_model = tf.keras.models.load_model(\n",
    "    \"MLM_Embed_indiv.h5\")\n",
    "gen_imgs = mlm_model.predict(x_test)\n",
    "\n",
    "for i in range(len(gen_imgs)):\n",
    "    gen_imgs[i] = np.reshape(gen_imgs[i],(6005,-1))\n",
    "\n",
    "gen_imgs = np.concatenate(gen_imgs,axis=1)\n",
    "resamples = wide_to_long(gen_imgs)\n",
    "\n",
    "\n",
    "# Evaluation for Home_income\n",
    "roc_auc_list = []\n",
    "for i in range(6):\n",
    "    r1 = gen_imgs[:,range(n_uni_col[i],n_uni_col[i+1])]\n",
    "    r1 = r1[weights[i]==1]\n",
    "    s1 = x_Bert_test.iloc[:,i][weights[i]==1].cat.codes\n",
    "    if s1.max() > 1:\n",
    "        roc_auc_list.append(metrics.roc_auc_score(s1,r1,multi_class=\"ovr\"))\n",
    "    else:\n",
    "        roc_auc_list.append(metrics.roc_auc_score(s1,r1[:,1]))\n",
    "print(np.mean(roc_auc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained bert model\n",
    "mlm_model = tf.keras.models.load_model(\n",
    "    \"MLM_Embed_indiv.h5\")\n",
    "\n",
    "embedding_layers =  mlm_model.layers[11:22]\n",
    "samples = np.array(x_test_cond.copy())\n",
    "\n",
    "\n",
    "def convert_to_embedding(samples):\n",
    "    samples_emb = []\n",
    "    for i in range(len(embedding_layers)):\n",
    "        emb_weight = embedding_layers[i].get_weights()[0]\n",
    "        trgt = samples[:,range(n_uni_col[i],n_uni_col[i+1])]\n",
    "        samples_emb.append(np.dot(trgt,emb_weight))\n",
    "    \n",
    "    return(np.concatenate(samples_emb,axis=1))\n",
    "\n",
    "samples_emb = convert_to_embedding(samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
