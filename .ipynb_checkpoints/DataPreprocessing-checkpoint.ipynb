{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the sample data\n",
    "   * Sampled small-scale complete data (trave survey data from 10,000 individuals)\n",
    "   * Sampled large-scale incomplete data (smart card data from 10,000 + 10,000 individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Load the sampled complete and incomplte data \n",
    "x_train_cond_R = pd.read_csv('Data/train_complete_qualitative.csv')\n",
    "y_train_cond_R = pd.read_csv('Data/train_complete_tripChain.csv')\n",
    "y_train_cond_SC_R = pd.read_csv('Data/train_incomplete_tripChain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64141, 9)\n",
      "(64141, 20)\n",
      "(112982, 20)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_cond_R.shape)\n",
    "print(y_train_cond_R.shape)\n",
    "print(y_train_cond_SC_R.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preprocessing\n",
    " * Transforming the long-form data into ndarray sequential form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remaining only relevent columns\n",
    "x_catcol = x_train_cond_R.columns.drop(['ID','P_Trip_seq'])\n",
    "y_catcol = y_train_cond_R.columns.drop(['ID','P_Trip_seq',\"JIGA\",\"P_Home_Meanage\",\"P_Home_Older\"])\n",
    "\n",
    "## Define data types\n",
    "x_train_cond_R[x_catcol]= x_train_cond_R[x_catcol].apply(lambda x: x.astype('category') )\n",
    "y_train_cond_R[y_catcol]= y_train_cond_R[y_catcol].apply(lambda x: x.astype('category'))\n",
    "y_train_cond_SC_R[y_catcol]= y_train_cond_SC_R[y_catcol].apply(lambda x: x.astype('category'))\n",
    "\n",
    "## Sort by ID and trip sequence (order)\n",
    "x_train_cond_R = x_train_cond_R.sort_values(by=['ID','P_Trip_seq'],axis=0)\n",
    "y_train_cond_R = y_train_cond_R.sort_values(by=['ID','P_Trip_seq'],axis=0)\n",
    "y_train_cond_SC_R = y_train_cond_SC_R.sort_values(by=['NID','P_Trip_seq'],axis=0)\n",
    "\n",
    "## Save the Trip purposes of each trip in the trip-chain\n",
    "\n",
    "samples = pd.concat([x_train_cond_R['ID'],x_train_cond_R[x_catcol].drop('P_Trip_purpose',axis=1)],axis=1)\n",
    "samples_R = x_train_cond_R.copy()\n",
    "samples_R['idx'] = samples_R.groupby('ID').cumcount()\n",
    "samples_R['prod_idx'] = 'TP_' + samples_R.idx.astype(str)\n",
    "\n",
    "Trip_purpose = samples_R.pivot(index='ID',columns='prod_idx',values='P_Trip_purpose')\n",
    "for col in Trip_purpose.columns:\n",
    "    Trip_purpose[col] = Trip_purpose[col].cat.add_categories(\"Z\").fillna(\"Z\")    \n",
    "samples =  pd.merge(samples.groupby('ID').head(1),Trip_purpose,on=\"ID\")\n",
    "\n",
    "\n",
    "## Create the ndarray trip purposes\n",
    "x_train_cond = pd.concat([x_train_cond_R['ID'],pd.get_dummies(x_train_cond_R[x_catcol].drop('P_Trip_purpose',axis=1))],axis=1)\n",
    "x_train_cond_R['idx'] = x_train_cond_R.groupby('ID').cumcount()\n",
    "x_train_cond_R['prod_idx'] = 'TP_' + x_train_cond_R.idx.astype(str)\n",
    "\n",
    "Trip_purpose = x_train_cond_R.pivot(index='ID',columns='prod_idx',values='P_Trip_purpose')\n",
    "for col in Trip_purpose.columns:\n",
    "    Trip_purpose[col] = Trip_purpose[col].cat.add_categories(\"Z\").fillna(\"Z\")\n",
    "    \n",
    "Trip_purpose = Trip_purpose[Trip_purpose['TP_1'] != 'Z']\n",
    "\n",
    "Trip_purpose = pd.get_dummies(Trip_purpose)\n",
    "x_train_cond =  pd.merge(x_train_cond.groupby('ID').head(1),Trip_purpose,on=\"ID\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide the trip-chain attributes into the sequential and non-sequential one\n",
    "\n",
    "y_train_cat = y_train_cond_R[['isHome','P_Arrival_time','stay_time','tr_time']]\n",
    "y_train_seq = pd.concat([pd.get_dummies(y_train_cat),y_train_cond_R[['P_Arrival_x','P_Arrival_y','ID','P_Trip_seq']]],axis=1)\n",
    "y_train_seq = y_train_seq[y_train_seq['ID'].isin(x_train_cond['ID'])]\n",
    "\n",
    "\n",
    "y_train_SC_cat = y_train_cond_SC_R[['isHome','P_Arrival_time','stay_time','tr_time']]\n",
    "y_train_SC_seq = pd.concat([pd.get_dummies(y_train_SC_cat),y_train_cond_SC_R[['P_Arrival_x','P_Arrival_y','NID','P_Trip_seq']]],axis=1)\n",
    "\n",
    "y_train_nseq = pd.concat([pd.get_dummies(y_train_cond_R[['Age_SC','start_time']]),y_train_cond_R[['ID','JIGA','P_Home_Meanage','P_Home_Older']]],axis=1)\n",
    "y_train_SC_nseq = pd.concat([pd.get_dummies(y_train_cond_SC_R[['Age_SC','start_time']]),y_train_cond_SC_R[['NID','JIGA','P_Home_Meanage','P_Home_Older']]],axis=1)\n",
    "\n",
    "y_train_nseq = y_train_nseq.groupby('ID').head(1)\n",
    "y_train_nseq = y_train_nseq[y_train_nseq['ID'].isin(x_train_cond['ID'])]\n",
    "y_train_nseq = y_train_nseq.drop(['ID'],axis=1)\n",
    "\n",
    "y_train_SC_nseq = y_train_SC_nseq.groupby('NID').head(1)\n",
    "y_train_SC_nseq = y_train_SC_nseq.drop(['NID'],axis=1)\n",
    "\n",
    "x_train_cond = x_train_cond.drop(['ID'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform the data into sequential ndarray\n",
    "\n",
    "### Zero padding\n",
    "def pad(x):\n",
    "    zero_data = np.zeros(shape=(maxlen - len(x),num_features+4))\n",
    "    d = pd.DataFrame(zero_data, columns=x.columns)\n",
    "    data = x.append(d, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "num_features = len(y_train_seq.columns)-4\n",
    "maxlen = 5\n",
    "num_data = y_train_seq['ID'].nunique()\n",
    "num_data_SC = y_train_SC_seq['NID'].nunique()\n",
    "\n",
    "### Adding dummy dimension to be divded 4 in the Transformer (for Multi-head attention)\n",
    "for i in range(3):\n",
    "    y_train_seq.insert(num_features,i,0)\n",
    "    y_train_SC_seq.insert(num_features,i,0)\n",
    "\n",
    "num_features = len(y_train_seq.columns)-4\n",
    "\n",
    "### Resahpe\n",
    "\n",
    "y_train_SC_seq=y_train_SC_seq.groupby('NID').apply(pad)\n",
    "y_train_SC_seq=y_train_SC_seq.to_numpy()\n",
    "y_train_SC_seq=y_train_SC_seq.reshape(num_data_SC,maxlen,num_features+4)\n",
    "\n",
    "y_train_seq=y_train_seq.groupby('ID').apply(pad)\n",
    "y_train_seq=y_train_seq.to_numpy()\n",
    "y_train_seq=y_train_seq.reshape(num_data,maxlen,num_features+4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the data as a numpy format\n",
    " * Split the data into training and test sets\n",
    " * Save the sequential data as ndarray format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def choice_train_test_split(X, y, y_ns, test_size=0.2,random_state=1004):\n",
    "    test_num = int(X.shape[0] * test_size)\n",
    "    train_num = X.shape[0] - test_num\n",
    "    np.random.seed(random_state)\n",
    "    train_idx = np.random.choice(X.shape[0], train_num, replace=False)\n",
    "    test_idx = np.setdiff1d(range(X.shape[0]), train_idx)\n",
    "    X_train = X.iloc[train_idx, :]\n",
    "    X_test = X.iloc[test_idx, :]\n",
    "    y_train = y[train_idx,:]\n",
    "    y_test = y[test_idx,:]\n",
    "    y_train_ns = y_ns.iloc[train_idx,:]\n",
    "    y_test_ns = y_ns.iloc[test_idx,:]     \n",
    "\n",
    "    return X_train, X_test, y_train, y_test,y_train_ns,y_test_ns\n",
    " \n",
    "X_train, X_test, y_train, y_test, y_train_ns, y_test_ns = choice_train_test_split(x_train_cond,y_train_seq,y_train_nseq,test_size=0.2,random_state=1004)\n",
    "y_train_SC, y_test_SC,y_train_SC_ns,y_test_SC_ns = train_test_split(y_train_SC_seq,y_train_SC_nseq,test_size=0.5,shuffle=True,random_state=1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Data/y_train_seq', y_train,allow_pickle=True)\n",
    "np.save('Data/y_test_seq', y_test,allow_pickle=True)\n",
    "np.save('Data/y_train_SC_seq', y_train_SC,allow_pickle=True)\n",
    "np.save('Data/y_test_SC_seq', y_test_SC,allow_pickle=True)\n",
    "\n",
    "X_train.to_csv('Data/x_train_cond.csv',index=False)\n",
    "X_test.to_csv('Data/x_test_cond.csv',index=False)\n",
    "y_train_ns.to_csv('Data/y_train_nseq.csv',index=False)\n",
    "y_test_ns.to_csv('Data/y_test_nseq.csv',index=False)\n",
    "y_train_SC_ns.to_csv('Data/y_train_SC_nseq.csv',index=False)\n",
    "y_test_SC_ns.to_csv('Data/y_test_SC_nseq.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
